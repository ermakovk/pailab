{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLRepo - A Quick Introduction\n",
    "In this notebook we give a quick introduction working with the repository and explain the basic priniples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import all things you need to get startes\n",
    "import pandas as pd\n",
    "import logging as logging\n",
    "\n",
    "# Here start the repository specific imports\n",
    "import pailab.repo as repo\n",
    "import pailab.memory_handler as memory_handler\n",
    "from pailab.repo_objects import RepoInfoKey, MeasureConfiguration\n",
    "from job_runner.job_runner import SimpleJobRunner, JobState, SQLiteJobRunner\n",
    "\n",
    "#You may set the loglevel and log-format here. \n",
    "#Note that the repository uses the logging module.\n",
    "#FORMAT = \"[%(filename)s:%(lineno)s - %(funcName)20s() ] %(message)s\"\n",
    "#logging.basicConfig(format=FORMAT, level=logging.DEBUG)\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data\n",
    "As an example machine learning task to ilustrate the way of working with the repository we use the Boston housing data from the  [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php) where we have applied some preprocessing. The data consists of house prices together with the house features `'RM'`, `'LSTAT'`, and `'PTRATIO'`:\n",
    "- `'RM'` is the average number of rooms among homes in the neighborhood.\n",
    "- `'LSTAT'` is the percentage of homeowners in the neighborhood considered \"lower class\" (working poor).\n",
    "- `'PTRATIO'` is the ratio of students to teachers in primary and secondary schools in the neighborhood.\n",
    "We just read the csv-file containing the data (also in the repository) into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('housing.csv')\n",
    "#data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new repository\n",
    "We first create a new repository for our task. The repository is the central key around all functionality is built. Similar to a repository used for source control in classical software development it contains all data and algorithms needed for the machine learning task. The repository needs storages for \n",
    "- scripts containing the machine learning algorithms and interfaces,\n",
    "- numerical objects such as arrays and matrices representing data, e.g. input data, data from the valuation of the models,\n",
    "- json documents representing parameters, e.g. training parameter, model parameter.\n",
    "\n",
    "To keep things simple, we just start using in memory storages. Note that the used memory interfaces are except for testing and playing around not be the first choice, since when ending the session, everything will be lost...\n",
    "\n",
    "In addition to the storages the repository needs a reference to a JobRunner which the platform can use to execute machine learning jobs. For this example we use the most simple one, executing everything sequential in the same thread, the repository runs in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# setting up the repository\n",
    "if True:\n",
    "    # for the scripts as well as for parameters we used the same simple in memory handler\n",
    "    handler = memory_handler.RepoObjectMemoryStorage() \n",
    "    # for the numerical objects (numpy) we use the simple NumpyMemoryStorage \n",
    "    numpy_handler = memory_handler.NumpyMemoryStorage()\n",
    "    # and for the sake of being simple, we use a SimpleJobRunner\n",
    "    job_runner = SimpleJobRunner(None)\n",
    "else:\n",
    "    from pailab.disk_handler import RepoObjectDiskStorage\n",
    "    from pailab.numpy_handler_hdf import NumpyHDFStorage\n",
    "    handler = RepoObjectDiskStorage('c:/temp/boston_housing_repo')\n",
    "    numpy_handler = NumpyHDFStorage('c:/temp/boston_housing_repo') \n",
    "    job_runner = SQLiteJobRunner('c:/temp/job_runner.sqlite', None)\n",
    "ml_repo = repo.MLRepo('test_user', handler, numpy_handler, handler, job_runner)\n",
    "job_runner.set_repo(ml_repo)\n",
    "ml_repo._job_runner = job_runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding data\n",
    "The data in the repository is handled by two different data objects:\n",
    "- RawData is the object containing real data.\n",
    "- DataSet is the object conaining the logical data, i.e. a reference to a RawData object together with a specification, which data from the RawData will be used. Here, one can specify a fixed version of the underlying RawData object (then changes to the RawData will not affect the derived DataSet) or a fixed or floating subset of the RawData by defininga start and endindex cutting the derived data just out of the original data.\n",
    "\n",
    "Normally one will add RawData and then define DataSets which are used to train or test a model which is exactly the way shown in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add RawData. A convenient way to add RawData is simply to use the method add_data.\n",
    "# This method just takes a pandas dataframe and the specification, which columns belong to the input \n",
    "#and which to the targets.\n",
    "ml_repo.add_data('boston_housing', data, input_variables=['RM', 'LSTAT', 'PTRATIO'], target_variables = ['MEDV'])\n",
    "# create DataSet objects for training and test data\n",
    "training_data = repo.DataSet('boston_housing', 0, 300, \n",
    "                            repo_info = {RepoInfoKey.NAME.value: 'training_data', RepoInfoKey.CATEGORY.value: repo.MLObjectType.TRAINING_DATA})\n",
    "test_data = repo.DataSet('boston_housing', 301, None, \n",
    "                            repo_info = {RepoInfoKey.NAME.value: 'test_data',  RepoInfoKey.CATEGORY.value: repo.MLObjectType.TEST_DATA})\n",
    "# add the objects to the repository. The method returns a dictionary of object names to version numbers of the added objects.\n",
    "version_list = ml_repo.add([training_data, test_data], message = 'add training and test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating the DataSet we have to set two important informations for the repository, given as a dictionary:\n",
    "- The object name. Each object in the repository needs to have a unique name in the repository.\n",
    "- The object type which gives. In our example here we say that we specify that the DataSet are training and test data. Note that on can have only one training data object pre repository while the repository can obtain many different test data sets.\n",
    "\n",
    "Some may wonder what is now stored in *version_list*.\n",
    "** Adding an object (independent if it is a data object or some other object such as a parameter), the object gets a version number and no object will be removed, adding just adds a new version.** The add method returns a dictionary of the object names together with their version number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(version_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a model\n",
    "The next step to do machine learning would be to define a model which will be used in the repository. A model consists of the following pieces\n",
    "- a skript where the code for the model valuation is defined together with the function name of the evaluation method\n",
    "- a skript where the code for the model training is defined together with th function nam of the training method\n",
    "- a model parameter object defining the model parameter and which must have implemented the correct interface so that it can be used within the repository (see the documentation on integrating new objects, normally there is not more to do then just simply add *@repo_object_init()* to the line above your *__init__* method)\n",
    "- a training parameter object defining training parameters (such as number of optimization steps etc.), if necessary for your algorithms (this oen is optional)\n",
    "\n",
    "** SKLearn models as an example**\n",
    "\n",
    "We do not have to define the pieces defined above, if we use the sklearn module. Instead we can use the externals.sklearn module interfacing \n",
    "the sklearn package so that this can be used within the repository. This interface provides a simple method (add_model) to add an arbitrary sklearn model as a model which can be handled by the repository. This method adds a bunch of repo objects to the repository (according to the pieces described above):\n",
    "- An object defining the function to be called to evaluate the model\n",
    "- An object defining the function to be called to train the model\n",
    "- An object defining the model\n",
    "- An object defining the model parameter\n",
    "For the following we just use a DecisionTree as our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import externals.sklearn_interface as sklearn_interface\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "sklearn_interface.add_model(ml_repo, DecisionTreeRegressor(), model_param={'max_depth': 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Now, model taining is very simple, since you have defined training and testing data as well as  methods to value and fit your model and the model parameter.\n",
    "So, you can just call *run_training* on the repository, and the training is perfomred automatically.\n",
    "The training job is executed via the JobRunner you specified setting up the repository. All method of the repository involving jobs return the job id when adding the job to the JobRunner so that you can control the status of the task and see if it sucessfully finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_id = ml_repo.run_training()  \n",
    "job_info = job_runner.get_info(job_id[0], job_id[1])\n",
    "#print(job_info.trace_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation\n",
    "To measure errors and to provide plots the model must be evaluated on all test and training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_id = ml_repo.run_evaluation()\n",
    "#info =job_runner.get_info(job_id[1]) \n",
    "#print(str(info.trace_back))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add and compute measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml_repo.add_measure(MeasureConfiguration.MAX)\n",
    "ml_repo.add_measure(MeasureConfiguration.R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_ids = ml_repo.run_measures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_measure = ml_repo._get('DecisionTreeRegressor/measure/training_data/max')\n",
    "print(str(max_measure.value))\n",
    "max_measure = ml_repo._get('DecisionTreeRegressor/measure/test_data/max')\n",
    "print(str(max_measure.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Working with the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in repo.MLObjectType:\n",
    "    names = ml_repo.get_names(k.value)\n",
    "    for n in names: \n",
    "        print(n + '\\t  ' + k.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ml_repo.get_commits():\n",
    "    print(str(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in job_runner._job_info.items():\n",
    "    print(str(k) + ':  ' + str(v))\n",
    "#job_runner.get_info('34484a2c-c225-11e8-9693-fc084a6691eb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change model parameter, check consistency and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = ml_repo._get('DecisionTreeRegressor/model_param')\n",
    "param.sklearn_params['max_depth'] = 2\n",
    "version = ml_repo.add(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pailab.tools as tools\n",
    "#depp = ml_repo._get('DecisionTreeRegressor/model_param')\n",
    "results = tools.check_model(ml_repo, 'DecisionTreeRegressor')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_repo.run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tools.check_model(ml_repo, 'DecisionTreeRegressor')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_repo.run_evaluation()\n",
    "ml_repo.run_measures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = ml_repo._get('DecisionTreeRegressor/measure/test_data/r2',version = (0,100))\n",
    "for x in measure:\n",
    "    print(str(x.repo_info))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_repo.get_names(repo.MLObjectType.MEASURE_CONFIGURATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = ml_repo._get('DecisionTreeRegressor/measure/training_data/r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(m.repo_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ml_repo._get('boston_housing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(data.repo_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append RawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ml_repo.get_training_data(full_object = False)\n",
    "print(train_data.repo_info[RepoInfoKey.NAME] +': ' +str(train_data))\n",
    "test_data = ml_repo.get_names(repo.MLObjectType.TEST_DATA)\n",
    "for k in test_data:\n",
    "    t = ml_repo._get(k)\n",
    "    print(str(t)+ ' Version: ' + str(t.repo_info[RepoInfoKey.VERSION]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "ml_repo.append_raw_data('boston_housing', x_data = array([[ 6.575, 4.98, 15.3]]), y_data =array([[504000.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.repo_info[RepoInfoKey.NAME] +': ' +str(train_data))\n",
    "for k in test_data:\n",
    "    t = ml_repo._get(k)\n",
    "    print(str(t) + ' Version: ' + str(t.repo_info[RepoInfoKey.VERSION]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tools.check_model(ml_repo, 'DecisionTreeRegressor')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Repo-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pailab.plot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(2):\n",
    "    training_data = ml_repo._get('training_data')\n",
    "    training_data.end_index += 50\n",
    "    ml_repo.add(training_data, message='add 50 datapoints to end_index')\n",
    "    for i in range(6,12):\n",
    "        #print(i)\n",
    "        param = ml_repo._get('DecisionTreeRegressor/model_param')\n",
    "        param.sklearn_params['max_depth'] = i\n",
    "        version = ml_repo.add(param)\n",
    "        ml_repo.add(param)\n",
    "        ml_repo.run_training()\n",
    "        ml_repo.run_evaluation()\n",
    "        ml_repo.run_measures()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "### Plot measures by parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pailab.plot_helper as plt_helper\n",
    "import pailab.plot as plot\n",
    "#if False:\n",
    "#print(pd.DataFrame(\n",
    "#plt_helper.get_measure_by_model_parameter(ml_repo, 'DecisionTreeRegressor/measure/test_data/r2', 'max_depth')\n",
    "#))\n",
    "plot.measure_by_model_parameter(ml_repo, 'DecisionTreeRegressor/measure/test_data/r2', 'max_depth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.histogram(ml_repo, 'test_data', x_coordinate = 'PTRATIO') #, y_coordinate='MEDV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "depp = ml_repo._get( 'training_data', version = (0,100))\n",
    "#print(str(depp))\n",
    "for x in depp:\n",
    "    print(str(x)+ ', version: ' + str(x.repo_info[RepoInfoKey.VERSION]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(str(depp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "786px",
    "left": "0px",
    "right": "1470.45px",
    "top": "65.9943px",
    "width": "260px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
